__Course__ : The Data Scientist ToolBox
__Week__ :3 

Video_1

__Type of Data Science Questions__ :

In approximate order of difficulty

* Descriptive
* Exploratory
* Inferential
* Predictive
* Causal
* Mechanistic

## About Descriptive Analyses

Goal: Describe a set of data

1. The first kind of data analysis performed.
2. Commonly applied to census data
3. The description and interpretation are different steps.
4. Descriptions can usually not be generalized without additional statistical modeling

Ex. Google Ngram

## About Exploratory Anaylsis

Goal: Find relationships you didn't know about

1. Exploratory models are good for discovering new connections.
2. They are also useful for defining future studies.
3. Exploratory analyses are usually not the final say.
4. Exploratory analyses alone should not be used for generalizing/predicting.
5. ##Correlation does not imply causation##.

## About Inferential analysis

Goal: Use a relatively small sample of data to say something about a bigger population.

1. Inference is commonly the goal of statistical models.
2. Inference involves estimating both the quantity you care about and your uncertainty about your estimate.
3. Inference depends heavily on both the population and the sampling scheme.

## About predictive analysis

Goal: To use the data on some objects to predict values for another object.

1. If X predicts Y it does not mean X causes Y.
2. Accurate prediction depends heavily on measuring the right variables.
3. Although there are better and worse prediction models, more data and a sample model works really well.
4. Prediction is very hard, especially about the future references.


## About causal analysis

Goal: To find out what happens to one variable when you make another variable change.

1. Usually randomized studies are required to identify causation.
2. There are approaches to inferring causation in non-randomized studies, but they are complicated and sensitive to assumptions.
3. Causal relationships are usually identified as average effects, but may not apply to every individual.
4. Causal models are usually the "gold standard" for data analysis.

## About Mechanistic analysis

Goal: Understand the exact changes in variables that elad to changes in other variabels for individual objects.

1. Incredibly hard to infer, expect in simple situations.
2. Usually modeled by a deterministic set of equations(physical/engineering science.)
3. Generally the random component of the data is measurement error.
4. If the equations are known but the parameters are not, they may be inferred with data analysis.

# What is data?

__Definition:__ "Data are values of qualitative or quantitative variables, belonging to a set of items."

**Set of items** : Sometimes call the population; the set of objects you are interested in.

Variables :A measurement or characteristic of an item.

The data is the second most important thing

1. The most important thing in data science is the question.
2. The second most important is the data.
3. Often the data will limit or enable the questions.
4. But having data can't save you if you don't have a question.

# What about Big Data?

Right Data is more imporatant than big data.
1.8 zettabytes in 2011 being created.
EMC2 , cloud and big data.
Study on small world problem ( seperation of six degrees)
Don't use hadoop if your data is not big enough.


Big or small - you need the right data

"The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data...."


## Basics of Experimental Design.

Example : - Genomic information to guide the use of chemtherapeutics. 
			Deriving chemosensitivy 
			Worry about satisitical method used 

			http://figshare.com

			http://github.com/jtleek/datasharing.

1. Formulate your question in advance.

Question: Does changing the text on your website improve donations?

Experiment:

1. Randomly show visitors one version or the other.
2. Measure how they donate.
3. Determine which is better.

Key idea of large component of data science i.e. Statistical inference.


Probability of showing the site to the user -> We have some sample which will derive a descriptive statistics which when inferential lead to being applied to the whole population.

Variability - Scenario 1 

less variability in results can leads to prediction.

Issue of confounding..

S(Shoe size) -> L(literacy )

S(Shoe size) -> Age -> Literacy

Age is the missing variable. So age is the confounder. The processing is known as confounding.


Coorelation is not causation.

How to deal with cofounding.

#### Randomization and blocking

1. If you can (and want to) fix a variable
	- Website always says Obama 2014 on it.

2. If you don't fix a variable, stratify it.
	- If you are tesing sign up phrases and have two website colors, use both phrases equally on both.

3. If you can't fix a variable, randomize it.

### Prediction

Probability/Sampling --> Training set --> prediction function f(sample) --> outcome(yes/no)

### Prediction versus inference.

Inference distributions have more overalapped area.
whereas prediction distributions has less overallped area.

Sensitivity --> Pr(positive test | disease)
Specificity --> Pr(negative test | no disease)
Positive Predictive Value -> Pr(disease | positive test)
Negative Predicitve value -> Pr(no disease | negative tesT)
Accuracy -> Pr(Correct Outcome)

### Beware data dredging.


Good experiments -- have replication, measure variability , generalize to the problem you care about , are transparent

Prediction is not inference - Both can be important

Beware data dredging.



